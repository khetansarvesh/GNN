{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jTCV511PgOR"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/GNN/blob/main/Node2Vec.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE7qwkLEHttb"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import Node2Vec\n",
        "import os.path as osp\n",
        "import torch\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "dataset = 'Cora'\n",
        "path = osp.join('.', 'data', dataset)\n",
        "dataset = Planetoid(path, dataset)  # dowload or load the Cora dataset\n",
        "data = dataset[0]\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # check if cuda is available to send the model and tensors to the GPU\n",
        "model = Node2Vec(data.edge_index, embedding_dim=128, walk_length=20,\n",
        "                 context_size=10, walks_per_node=10,\n",
        "                 num_negative_samples=1, p=1, q=1, sparse=True).to(device)\n",
        "\n",
        "loader = model.loader(batch_size=128, shuffle=True, num_workers=4)  # data loader to speed the train\n",
        "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)  # initzialize the optimizer\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()  # put model in train model\n",
        "    total_loss = 0\n",
        "    for pos_rw, neg_rw in tqdm(loader):\n",
        "        optimizer.zero_grad()  # set the gradients to 0\n",
        "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))  # compute the loss for the batch\n",
        "        loss.backward()\n",
        "        optimizer.step()  # optimize the parameters\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "for epoch in range(1, 100):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
        "\n",
        "all_vectors = \"\"\n",
        "for tensor in model(torch.arange(data.num_nodes, device=device)):\n",
        "    s = \"\\t\".join([str(value) for value in tensor.detach().cpu().numpy()])\n",
        "    all_vectors += s + \"\\n\"\n",
        "# save the vectors\n",
        "with open(\"vectors.txt\", \"w\") as f:\n",
        "    f.write(all_vectors)\n",
        "# save the labels\n",
        "with open(\"labels.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join([str(label) for label in data.y.numpy()]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "from karateclub.estimator import Estimator\n",
        "from karateclub.utils.walker import BiasedRandomWalker\n",
        "\n",
        "\n",
        "class Node2Vec(Estimator):\n",
        "    r\"\"\"An implementation of `\"Node2Vec\" <https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf>`_\n",
        "    from the KDD '16 paper \"node2vec: Scalable Feature Learning for Networks\".\n",
        "    The procedure uses biased second order random walks to approximate the pointwise mutual information\n",
        "    matrix obtained by pooling normalized adjacency matrix powers. This matrix\n",
        "    is decomposed by an approximate factorization technique.\n",
        "\n",
        "    Args:\n",
        "        walk_number (int): Number of random walks. Default is 10.\n",
        "        walk_length (int): Length of random walks. Default is 80.\n",
        "        p (float): Return parameter (1/p transition probability) to move towards from previous node.\n",
        "        q (float): In-out parameter (1/q transition probability) to move away from previous node.\n",
        "        dimensions (int): Dimensionality of embedding. Default is 128.\n",
        "        workers (int): Number of cores. Default is 4.\n",
        "        window_size (int): Matrix power order. Default is 5.\n",
        "        epochs (int): Number of epochs. Default is 1.\n",
        "        learning_rate (float): HogWild! learning rate. Default is 0.05.\n",
        "        min_count (int): Minimal count of node occurrences. Default is 1.\n",
        "        seed (int): Random seed value. Default is 42.\n",
        "    \"\"\"\n",
        "    _embedding: List[np.ndarray]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        walk_number: int = 10,\n",
        "        walk_length: int = 80,\n",
        "        p: float = 1.0,\n",
        "        q: float = 1.0,\n",
        "        dimensions: int = 128,\n",
        "        workers: int = 4,\n",
        "        window_size: int = 5,\n",
        "        epochs: int = 1,\n",
        "        learning_rate: float = 0.05,\n",
        "        min_count: int = 1,\n",
        "        seed: int = 42,\n",
        "    ):\n",
        "        super(Node2Vec, self).__init__()\n",
        "\n",
        "        self.walk_number = walk_number\n",
        "        self.walk_length = walk_length\n",
        "        self.p = p\n",
        "        self.q = q\n",
        "        self.dimensions = dimensions\n",
        "        self.workers = workers\n",
        "        self.window_size = window_size\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.min_count = min_count\n",
        "        self.seed = seed\n",
        "\n",
        "    def fit(self, graph: nx.classes.graph.Graph):\n",
        "        \"\"\"\n",
        "        Fitting a DeepWalk model.\n",
        "\n",
        "        Arg types:\n",
        "            * **graph** *(NetworkX graph)* - The graph to be embedded.\n",
        "        \"\"\"\n",
        "        self._set_seed()\n",
        "        graph = self._check_graph(graph)\n",
        "        walker = BiasedRandomWalker(self.walk_length, self.walk_number, self.p, self.q)\n",
        "        walker.do_walks(graph)\n",
        "\n",
        "        model = Word2Vec(\n",
        "            walker.walks,\n",
        "            hs=1,\n",
        "            alpha=self.learning_rate,\n",
        "            epochs=self.epochs,\n",
        "            vector_size=self.dimensions,\n",
        "            window=self.window_size,\n",
        "            min_count=self.min_count,\n",
        "            workers=self.workers,\n",
        "            seed=self.seed,\n",
        "        )\n",
        "\n",
        "        n_nodes = graph.number_of_nodes()\n",
        "        self._embedding = [model.wv[str(n)] for n in range(n_nodes)]\n",
        "\n",
        "\n",
        "    def get_embedding(self) -> np.array:\n",
        "        r\"\"\"Getting the node embedding.\n",
        "\n",
        "        Return types:\n",
        "            * **embedding** *(Numpy array)* - The embedding of nodes.\n",
        "        \"\"\"\n",
        "        return np.array(self._embedding)\n"
      ],
      "metadata": {
        "id": "gkzlif8THujc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "h1233VS-CJ18",
        "xvcGmGqcCFqr",
        "j7VMr1LqB-ts",
        "ty0r2C3WEw00",
        "NEm0b3PCcgj7",
        "lhJYtenTje8-",
        "10F8WfeHxgWr",
        "vf_KuYZmnRgn",
        "plirCWZL3RAQ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}